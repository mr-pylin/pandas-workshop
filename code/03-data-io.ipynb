{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f7a74ec",
      "metadata": {},
      "source": [
        "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
        "    <div style=\"text-align: left; flex: 4\">\n",
        "        <strong>Author:</strong> Amirhossein Heydari ‚Äî \n",
        "        üìß <a href=\"mailto:amirhosseinheydari78@gmail.com\">amirhosseinheydari78@gmail.com</a> ‚Äî \n",
        "        üêô <a href=\"https://github.com/mr-pylin/pandas-workshop\" target=\"_blank\" rel=\"noopener\">github.com/mr-pylin</a>\n",
        "    </div>\n",
        "    <div style=\"text-align: right; flex: 1;\">\n",
        "        <a href=\"https://pandas.pydata.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
        "            <img src=\"../assets/images/pandas/logo/pandas_white.svg\" \n",
        "                 alt=\"Pandas Logo\"\n",
        "                 style=\"max-height: 48px; width: auto; background-color: #1f1f1f; border-radius: 8px;\">\n",
        "        </a>\n",
        "    </div>\n",
        "</div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "032095a7",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Dependencies](#toc1_)    \n",
        "- [Data Input and Output (I/O)](#toc2_)    \n",
        "  - [CSV Files](#toc2_1_)    \n",
        "    - [Reading CSV](#toc2_1_1_)    \n",
        "    - [Writing CSV](#toc2_1_2_)    \n",
        "    - [Chunked Reading for Large Files](#toc2_1_3_)    \n",
        "  - [Excel Files](#toc2_2_)    \n",
        "    - [Reading Excel](#toc2_2_1_)    \n",
        "    - [Writing Excel](#toc2_2_2_)    \n",
        "  - [JSON Files](#toc2_3_)    \n",
        "    - [Reading JSON](#toc2_3_1_)    \n",
        "    - [Writing JSON](#toc2_3_2_)    \n",
        "    - [Handling Nested JSON](#toc2_3_3_)    \n",
        "  - [Binary and Columnar Formats](#toc2_4_)    \n",
        "    - [Parquet](#toc2_4_1_)    \n",
        "    - [HDF5](#toc2_4_2_)    \n",
        "  - [Databases](#toc2_5_)    \n",
        "    - [Using SQLAlchemy](#toc2_5_1_)    \n",
        "    - [Read SQL Queries into DataFrames](#toc2_5_2_)    \n",
        "    - [Write DataFrames to Databases](#toc2_5_3_)    \n",
        "  - [Remote and Online Data](#toc2_6_)    \n",
        "    - [Read from URLs](#toc2_6_1_)    \n",
        "    - [Fetching Data from APIs](#toc2_6_2_)    \n",
        "  - [Performance and Best Practices](#toc2_7_)    \n",
        "    - [Memory Usage Considerations](#toc2_7_1_)    \n",
        "    - [Choosing the Right File Format](#toc2_7_2_)    \n",
        "    - [Compression and Storage Trade-offs](#toc2_7_3_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e73df675",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Dependencies](#toc0_)\n",
        "\n",
        "- You have to install **optional dependencies** in order to run this notebook!\n",
        "- Visit [**README.md**](../README.md) file for more info and instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ec40e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import sqlalchemy as sa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745aa211",
      "metadata": {},
      "outputs": [],
      "source": [
        "# disable wrapping entirely\n",
        "pd.set_option(\"display.expand_frame_repr\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289ce3a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read path\n",
        "CSV_URL = r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/movies/csv/dataset.csv\"\n",
        "EXCEL_URL = r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/movies/excel/dataset.xlsx\"\n",
        "JSON_RECORDS_URL = r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/movies/json/dataset_records.json\"\n",
        "JSON_COLUMNS_URL = r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/movies/json/dataset_columns.json\"\n",
        "JSON_LINES_URL = r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/movies/json/dataset_lines.json\"\n",
        "PARQUET_URL = r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/movies/parquet/dataset.parquet\"\n",
        "HDF5_URL = r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/movies/hdf5/dataset.h5\"\n",
        "SQL_URL = r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/movies/sql/dataset.db\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f247d73d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# write path\n",
        "WRITE_PATH = Path(\"../assets/datasets\")\n",
        "WRITE_PATH.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fd700d8",
      "metadata": {},
      "source": [
        "# <a id='toc2_'></a>[Data Input and Output (I/O)](#toc0_)\n",
        "\n",
        "<div style=\"text-align: center; padding-top: 10px;\">\n",
        "    <img src=\"../assets/images/pandas/tutorials/02/io_readwrite.svg\" alt=\"Pandas Series\" style=\"min-width: 256px; max-height: 40%; width: auto; background-color: #DBDBDB; border-radius: 16px;\">\n",
        "    <p><em>Figure 1: Pandas Data Input and Output</em> (<a href=\"https://pandas.pydata.org/docs/getting_started/index.html\" target=\"_blank\">source</a>)</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa978c3d",
      "metadata": {},
      "source": [
        "## <a id='toc2_1_'></a>[CSV Files](#toc0_)\n",
        "\n",
        "CSV (Comma-Separated Values) is the most common format for **tabular data exchange**. Pandas provides flexible tools to both **read** and **write** CSV files.\n",
        "\n",
        "üìù **Docs**:\n",
        "\n",
        "- `pandas.read_csv`: [pandas.pydata.org/docs/reference/api/pandas.read_csv.html](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
        "- `pandas.DataFrame.to_csv`: [pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b24ee6c6",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_1_'></a>[Reading CSV](#toc0_)\n",
        "\n",
        "- Use **`read_csv`** to load CSV files into a `DataFrame`.\n",
        "- Supports different delimiters (`,`, `;`, `\\t`, etc.).\n",
        "- Can handle:\n",
        "  - Custom column names\n",
        "  - Skipping rows or headers\n",
        "  - Selecting specific columns (`usecols`)\n",
        "  - Forcing data types (`dtype`)\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- CSVs often come from different systems with different conventions.\n",
        "- Properly handling headers, separators, and encoding avoids **data corruption** and **silent errors**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25552a71",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load full dataset\n",
        "df_from_csv_1 = pd.read_csv(CSV_URL)\n",
        "\n",
        "# log\n",
        "df_from_csv_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6da8300",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load specific columns only\n",
        "df_from_csv_2 = pd.read_csv(CSV_URL, usecols=[\"Film\", \"Genre\"])\n",
        "\n",
        "# log\n",
        "df_from_csv_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51598aa4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# forcing specific dtypes\n",
        "df_from_csv_3 = pd.read_csv(CSV_URL, dtype={\"Year\": \"string\"})\n",
        "\n",
        "# log\n",
        "df_from_csv_3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35e8f20",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_2_'></a>[Writing CSV](#toc0_)\n",
        "\n",
        "- Use **`to_csv`** to export a `DataFrame` back into a CSV.\n",
        "- Options include:\n",
        "  - Saving with or without the index\n",
        "  - Choosing delimiters (`,` vs. `;`)\n",
        "  - Applying compression (`gzip`, `bz2`, `zip`)\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- File size and compatibility depend on these options.\n",
        "- Compressed CSVs are useful for large datasets without losing readability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38bbcadb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# save DataFrame to CSV (without index)\n",
        "df_from_csv_1.to_csv(WRITE_PATH / \"dataset_1.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1cffaed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# save with a custom delimiter\n",
        "df_from_csv_2.to_csv(WRITE_PATH / \"dataset_2.csv\", sep=\";\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "188e60f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# save compressed CSV\n",
        "df_from_csv_3.to_csv(WRITE_PATH / \"dataset_3.csv.gz\", index=False, compression=\"gzip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7100ae14",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_3_'></a>[Chunked Reading for Large Files](#toc0_)\n",
        "\n",
        "- CSVs can be **too large to fit into memory**.\n",
        "- Pandas allows **chunked reading** with the `chunksize` parameter.\n",
        "- This returns an iterator of smaller `DataFrame` objects.\n",
        "\n",
        "üìà **Use cases**:  \n",
        "- Incremental aggregation (e.g., sum, mean, counts).\n",
        "- Filtering and storing only relevant rows.\n",
        "- Memory-efficient ETL workflows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a68f386",
      "metadata": {},
      "outputs": [],
      "source": [
        "# process large CSV in chunks of 10 rows\n",
        "chunk_iter = pd.read_csv(CSV_URL, chunksize=10)\n",
        "\n",
        "total_rows = 0\n",
        "for i, chunk in enumerate(chunk_iter, start=1):\n",
        "    total_rows += len(chunk)\n",
        "    mem_kb = chunk.memory_usage(deep=True).sum() / 1024\n",
        "    print(f\"chunk {i}: {len(chunk):>2} rows, ~{mem_kb:.2f} KB in memory\")\n",
        "\n",
        "print(f\"total rows processed: {total_rows}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58682ae7",
      "metadata": {},
      "source": [
        "## <a id='toc2_2_'></a>[Excel Files](#toc0_)\n",
        "\n",
        "- Excel is a widely used format for **business and analytical data**.\n",
        "- Pandas supports both reading and writing Excel files, though it relies on **third-party libraries** such as `openpyxl` or `xlsxwriter`.\n",
        "\n",
        "üìù **Docs**:\n",
        "\n",
        "- `pandas.read_excel`: [pandas.pydata.org/docs/reference/api/pandas.read_excel.html](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html)\n",
        "- `pandas.DataFrame.to_excel`: [pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f3ce65a",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_1_'></a>[Reading Excel](#toc0_)\n",
        "\n",
        "- Use **`read_excel`** to load Excel files into a `DataFrame`.\n",
        "- Supports `.xls` and `.xlsx` formats.\n",
        "- Can handle:\n",
        "  - Selecting specific sheets (`sheet_name`)\n",
        "  - Reading multiple sheets at once (returns a dict of DataFrames)\n",
        "  - Choosing specific columns or rows\n",
        "  - Managing missing headers or custom column names\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- Real-world Excel files often contain **multiple sheets**, **merged cells**, or **metadata rows**.\n",
        "- Knowing how to select the right sheet and columns prevents **messy data imports**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85bfa9a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read a single sheet\n",
        "df_from_excel_1 = pd.read_excel(EXCEL_URL, sheet_name=\"Sheet1\")\n",
        "\n",
        "# log\n",
        "df_from_excel_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e27075",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read multiple sheets at once (returns dict of DataFrames)\n",
        "df_from_excel_2 = pd.read_excel(EXCEL_URL, sheet_name=None)\n",
        "\n",
        "# log\n",
        "print(f\"df_from_excel_2.keys(): {df_from_excel_2.keys()}\\n\")\n",
        "print(f\"df_from_excel_2['Top10']:\\n{df_from_excel_2['Top10']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e047d4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read with specific columns\n",
        "df_from_excel_3 = pd.read_excel(EXCEL_URL, sheet_name=\"Sheet1\", usecols=[\"Film\", \"Genre\"])\n",
        "\n",
        "# log\n",
        "df_from_excel_3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c31a5e05",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_2_'></a>[Writing Excel](#toc0_)\n",
        "\n",
        "- Use **`to_excel`** to export a `DataFrame` into Excel format.\n",
        "- Options include:\n",
        "  - Choosing sheet names\n",
        "  - Writing multiple DataFrames into one file (using `ExcelWriter`)\n",
        "  - Preserving formatting with engines like `openpyxl` or `xlsxwriter`\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:  \n",
        "- Excel is still a standard format for **reporting and sharing results**.\n",
        "- Controlling formatting (e.g., column widths, styles) can make exported files more **user-friendly**.\n",
        "- **Engine choice**:\n",
        "  - `openpyxl`: read/write support, good for general use and preserving existing files.  \n",
        "  - `xlsxwriter`: write-only, faster, and better for advanced formatting and large exports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241859ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# write a DataFrame to a single sheet\n",
        "df_from_excel_1.to_excel(WRITE_PATH / \"dataset_1.xlsx\", sheet_name=\"Movies\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db73cc72",
      "metadata": {},
      "outputs": [],
      "source": [
        "# write multiple DataFrames to one Excel file\n",
        "with pd.ExcelWriter(WRITE_PATH / \"dataset_2.xlsx\", engine=\"openpyxl\") as writer:\n",
        "    df_from_excel_1.to_excel(writer, sheet_name=\"Movies\", index=False)\n",
        "    df_from_excel_2[\"Top10\"].to_excel(writer, sheet_name=\"Top10\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21553e38",
      "metadata": {},
      "source": [
        "## <a id='toc2_3_'></a>[JSON Files](#toc0_)\n",
        "\n",
        "- JSON (JavaScript Object Notation) is a lightweight, flexible format for **storing structured data**.\n",
        "- It is widely used for **web APIs** and **data interchange**.\n",
        "- Pandas provides functions to seamlessly convert between JSON and DataFrames.\n",
        "\n",
        "üìù **Docs**:\n",
        "\n",
        "- `pandas.read_json`: [pandas.pydata.org/docs/reference/api/pandas.read_json.html](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html)\n",
        "- `pandas.DataFrame.to_json`: [pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html)\n",
        "- `pandas.json_normalize`: [pandas.pydata.org/docs/reference/api/pandas.json_normalize.html](https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4970fb08",
      "metadata": {},
      "source": [
        "### <a id='toc2_3_1_'></a>[Reading JSON](#toc0_)\n",
        "\n",
        "- Use **`read_json`** to load JSON into a `DataFrame`.\n",
        "- Supports different JSON formats:\n",
        "  - **Records (row-oriented):** list of dicts\n",
        "  - **Columns (column-oriented):** dict of lists\n",
        "  - **Index (index-oriented):** dict of dicts\n",
        "- Can handle JSON strings, local files, or remote URLs.\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- JSON can represent the same data in **different orientations**.\n",
        "- Understanding the structure is critical to avoid **parsing errors** or **unexpected layouts**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c56efe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read JSON in records (row-oriented) format\n",
        "df_from_json_1 = pd.read_json(JSON_RECORDS_URL, orient=\"records\")\n",
        "\n",
        "# log\n",
        "df_from_json_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe373c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read JSON in columns (column-oriented) format\n",
        "df_from_json_2 = pd.read_json(JSON_COLUMNS_URL, orient=\"columns\")\n",
        "\n",
        "# log\n",
        "df_from_json_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49aa8300",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read JSON in lines format\n",
        "df_from_json_3 = pd.read_json(JSON_LINES_URL, orient=\"records\", lines=True)\n",
        "\n",
        "# log\n",
        "df_from_json_3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0c6623",
      "metadata": {},
      "source": [
        "### <a id='toc2_3_2_'></a>[Writing JSON](#toc0_)\n",
        "\n",
        "- Use **`to_json`** to export a DataFrame into JSON.\n",
        "- Options include:\n",
        "  - Orientations: `records`, `split`, `index`, `columns`, `values`\n",
        "  - Line-delimited JSON for streaming (`lines=True`)\n",
        "  - Compression (`gzip`, `bz2`, etc.)\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- APIs often expect JSON in a **specific orientation**.\n",
        "- Line-delimited JSON is common for **big data pipelines**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57e131d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# export DataFrame to JSON (records orientation)\n",
        "df_from_json_1.to_json(WRITE_PATH / \"dataset_records_1.json\", orient=\"records\", indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d71840",
      "metadata": {},
      "outputs": [],
      "source": [
        "# export to JSON (index orientation)\n",
        "df_from_json_1.to_json(WRITE_PATH / \"dataset_index_1.json\", orient=\"index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3ef9327",
      "metadata": {},
      "outputs": [],
      "source": [
        "# export line-delimited JSON\n",
        "df_from_json_1.to_json(WRITE_PATH / \"dataset_lines_1.json\", orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b51299a",
      "metadata": {},
      "source": [
        "### <a id='toc2_3_3_'></a>[Handling Nested JSON](#toc0_)\n",
        "\n",
        "- Real-world JSON often has **nested structures** (dicts inside dicts or lists).\n",
        "- Pandas provides **`json_normalize`** to flatten nested data into tabular format.\n",
        "\n",
        "üìà **Use cases**:\n",
        "- API responses with hierarchical objects.\n",
        "- Event logs with nested metadata.\n",
        "- Converting semi-structured JSON into clean DataFrames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eedf3f12",
      "metadata": {},
      "outputs": [],
      "source": [
        "nested_data = {\n",
        "    \"store\": {\n",
        "        \"books\": [\n",
        "            {\n",
        "                \"title\": \"Book A\",\n",
        "                \"price\": 12.99,\n",
        "                \"authors\": [\n",
        "                    {\"name\": \"Alice\", \"age\": 30},\n",
        "                    {\"name\": \"Bob\", \"age\": 25},\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"title\": \"Book B\",\n",
        "                \"price\": 8.99,\n",
        "                \"authors\": [\n",
        "                    {\"name\": \"Charlie\", \"age\": 40},\n",
        "                ],\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    \"location\": \"Downtown\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05433c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# flatten nested JSON into tabular format\n",
        "df_nested = pd.json_normalize(\n",
        "    nested_data,\n",
        "    record_path=[\"store\", \"books\", \"authors\"],  # list to explode\n",
        "    meta=[[\"store\", \"books\", \"title\"], [\"store\", \"books\", \"price\"], \"location\"],  # parent info\n",
        ")\n",
        "\n",
        "# log\n",
        "df_nested.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79ed587c",
      "metadata": {},
      "source": [
        "## <a id='toc2_4_'></a>[Binary and Columnar Formats](#toc0_)\n",
        "\n",
        "- For large datasets and efficient storage, **binary and columnar formats** are often preferred over CSV or Excel.\n",
        "- Pandas provides strong support for modern formats like **Parquet** and **HDF5**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3832a6a0",
      "metadata": {},
      "source": [
        "### <a id='toc2_4_1_'></a>[Parquet](#toc0_)\n",
        "\n",
        "- **Parquet** is a columnar storage format optimized for:\n",
        "  - **Compression** (smaller file sizes)\n",
        "  - **Fast reads** (especially for subsets of columns)\n",
        "  - **Big data workflows** (commonly used with Spark, Dask, etc.)\n",
        "\n",
        "- Pandas supports reading and writing Parquet using libraries such as **`pyarrow`** or **`fastparquet`**.\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- Columnar storage is significantly faster than row-based formats (like CSV) for **analytical queries**.\n",
        "- Widely adopted in **data engineering pipelines** and **cloud storage**.\n",
        "\n",
        "üìù **Docs**:\n",
        "- `pandas.read_parquet`: [pandas.pydata.org/docs/reference/api/pandas.read_parquet.html](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html)\n",
        "- `DataFrame.to_parquet`: [pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "564e04af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read Parquet file\n",
        "df_from_parquet_1 = pd.read_parquet(PARQUET_URL, engine=\"pyarrow\")\n",
        "\n",
        "# log\n",
        "df_from_parquet_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "358ea2b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read specific columns only\n",
        "df_from_parquet_2 = pd.read_parquet(PARQUET_URL, columns=[\"Film\", \"Genre\"])\n",
        "\n",
        "# log\n",
        "df_from_parquet_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47915c97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# write DataFrame to Parquet\n",
        "df_from_parquet_1.to_parquet(WRITE_PATH / \"dataset.parquet\", engine=\"pyarrow\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "844bd3b7",
      "metadata": {},
      "source": [
        "### <a id='toc2_4_2_'></a>[HDF5](#toc0_)\n",
        "\n",
        "- **HDF5 (Hierarchical Data Format)** is designed for storing **large numerical datasets**.\n",
        "- Pandas integrates with **PyTables** to support HDF5 files.\n",
        "- Features:\n",
        "  - Hierarchical storage (like a file system inside a file)\n",
        "  - Efficient I/O for large arrays and time series\n",
        "  - Append mode for incremental writes\n",
        "\n",
        "üìà **Use cases**:\n",
        "- Storing intermediate results of large computations.\n",
        "- Handling time series data efficiently.\n",
        "- Scientific and engineering applications.\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- HDF5 is highly efficient for **single-machine workflows**.\n",
        "- Parquet is more common for **distributed/cloud-based pipelines**.\n",
        "- Choosing the right format depends on **workflow and scalability needs**.\n",
        "\n",
        "üìù **Docs**:\n",
        "\n",
        "- `pandas.read_hdf`: [pandas.pydata.org/docs/reference/api/pandas.read_hdf.html](https://pandas.pydata.org/docs/reference/api/pandas.read_hdf.html)\n",
        "- `DataFrame.to_hdf`: [pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_hdf.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_hdf.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6cbae1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# download a .h5 file\n",
        "HDF5_PATH = WRITE_PATH / \"dataset.h5\"\n",
        "urllib.request.urlretrieve(HDF5_URL, HDF5_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f99a5202",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read from HDF5\n",
        "df_from_hdf_1 = pd.read_hdf(HDF5_PATH)\n",
        "\n",
        "# log\n",
        "df_from_hdf_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2c185ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# write DataFrame to HDF5\n",
        "df_from_hdf_1.to_hdf(WRITE_PATH / \"dataset_1.h5\", key=\"movies\", mode=\"w\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9540c66a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# append new data to the same file\n",
        "df_from_hdf_1.head(10).to_hdf(WRITE_PATH / \"dataset_1.h5\", key=\"top10\", mode=\"a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00515db5",
      "metadata": {},
      "source": [
        "## <a id='toc2_5_'></a>[Databases](#toc0_)\n",
        "\n",
        "- Pandas can interact directly with **SQL databases**, allowing you to read from and write to relational databases using **SQLAlchemy** or database connectors.\n",
        "- This enables seamless integration with **production data pipelines** and **analytics workflows**.\n",
        "\n",
        "üìù **Docs**:  \n",
        "- `pandas.read_sql`: [pandas.pydata.org/docs/reference/api/pandas.read_sql.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html)\n",
        "- `pandas.read_sql_table`: [pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html)\n",
        "- `pandas.DataFrame.to_sql`: [pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html)\n",
        "- SQLAlchemy documentation: [sqlalchemy.org/docs](https://www.sqlalchemy.org/docs/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a70ee2b",
      "metadata": {},
      "source": [
        "### <a id='toc2_5_1_'></a>[Using SQLAlchemy](#toc0_)\n",
        "\n",
        "- **SQLAlchemy** is a Python library for **database abstraction**, supporting multiple database engines (SQLite, PostgreSQL, MySQL, etc.).\n",
        "- Pandas leverages SQLAlchemy to establish **connections** and perform **CRUD operations** on databases.\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- SQLAlchemy allows consistent handling of different databases.\n",
        "- Avoids writing engine-specific SQL for common tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4413c87d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# download a .db file\n",
        "SQL_PATH = WRITE_PATH / \"dataset.db\"\n",
        "TABLE_NAME = \"movies\"\n",
        "urllib.request.urlretrieve(SQL_URL, SQL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d5b57c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create an SQLite engine (file-based database)\n",
        "# url format: dialect+driver://username:password@host:port/database\n",
        "engine = sa.create_engine(f\"sqlite:///{str(SQL_PATH)}\")\n",
        "\n",
        "# find available tables\n",
        "table_names = sa.inspect(engine).get_table_names()\n",
        "print(\"Tables in the database:\", table_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6358b05d",
      "metadata": {},
      "source": [
        "### <a id='toc2_5_2_'></a>[Read SQL Queries into DataFrames](#toc0_)\n",
        "\n",
        "- Use **`read_sql_query`** or **`read_sql_table`** to fetch data from a database into a `DataFrame`.\n",
        "- Supports filtering via SQL queries before importing, reducing memory usage.\n",
        "- Can handle large tables efficiently with **chunked reading**.\n",
        "\n",
        "üìà **Use cases**:\n",
        "- Analytics on production databases without exporting CSV.\n",
        "- Incremental ETL pipelines.\n",
        "- Combining multiple tables via SQL joins before loading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef45b877",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read entire table into a DataFrame\n",
        "df_from_sql_1 = pd.read_sql_table(TABLE_NAME, con=engine)\n",
        "\n",
        "# log\n",
        "df_from_sql_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b6b4ea0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# run a custom SQL query\n",
        "query = \"SELECT Film,Genre FROM movies WHERE [Audience score %] > 85\"\n",
        "df_from_sql_2 = pd.read_sql_query(query, con=engine)\n",
        "\n",
        "# log\n",
        "df_from_sql_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e597099f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read large table in chunks\n",
        "chunk_iter = pd.read_sql_query(\"SELECT * FROM movies\", con=engine, chunksize=10)\n",
        "\n",
        "total_rows = 0\n",
        "for i, chunk in enumerate(chunk_iter, start=1):\n",
        "    total_rows += len(chunk)\n",
        "    mem_kb = chunk.memory_usage(deep=True).sum() / 1024\n",
        "    print(f\"chunk {i}: {len(chunk):>2} rows, ~{mem_kb:.2f} KB in memory\")\n",
        "\n",
        "print(f\"total rows processed: {total_rows}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "493a4d7c",
      "metadata": {},
      "source": [
        "### <a id='toc2_5_3_'></a>[Write DataFrames to Databases](#toc0_)\n",
        "\n",
        "- Use **`to_sql`** to save a `DataFrame` to a SQL database.\n",
        "- Options include:\n",
        "  - Replacing or appending to existing tables\n",
        "  - Specifying primary keys and indexes\n",
        "  - Chunked writes for large datasets\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- Enables automated reporting pipelines.\n",
        "- Integrates Pandas workflows with relational databases.\n",
        "- Ensures data persistence for downstream applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aab3884",
      "metadata": {},
      "outputs": [],
      "source": [
        "# write a DataFrame to SQL (replace existing table)\n",
        "engine = sa.create_engine(f\"sqlite:///../assets/datasets/dataset_1.db\")\n",
        "df_from_sql_1.to_sql(\"movies\", con=engine, if_exists=\"replace\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6d3d29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# append new rows to existing table\n",
        "df_from_sql_1.to_sql(\"movies\", con=engine, if_exists=\"append\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b504e6a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# write with explicit data types (example for SQLite/Postgres/MySQL)\n",
        "df_from_sql_1.to_sql(\n",
        "    \"movies\",\n",
        "    con=engine,\n",
        "    if_exists=\"replace\",\n",
        "    index=False,\n",
        "    dtype={\"Year\": sa.types.Float()},  # type: ignore\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f13a739a",
      "metadata": {},
      "source": [
        "## <a id='toc2_6_'></a>[Remote and Online Data](#toc0_)\n",
        "\n",
        "Pandas allows you to load data directly from **remote sources**, such as URLs or APIs, making it easy to integrate with **web-based datasets** and **online data pipelines**.\n",
        "\n",
        "üìù **Docs**:\n",
        "\n",
        "- requests library: [docs.python-requests.org](https://docs.python-requests.org/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cd3f0ea",
      "metadata": {},
      "source": [
        "### <a id='toc2_6_1_'></a>[Read from URLs](#toc0_)\n",
        "\n",
        "- Many functions like `read_csv`, `read_json`, and `read_excel` support **HTTP/HTTPS URLs** directly.\n",
        "- This eliminates the need to **download files manually** before processing.\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- Simplifies workflows for public datasets (Kaggle, government data portals).\n",
        "- Ensures you always access the **latest version** of the data.\n",
        "- Requires attention to **network reliability** and **file size**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c75695d",
      "metadata": {},
      "source": [
        "### <a id='toc2_6_2_'></a>[Fetching Data from APIs](#toc0_)\n",
        "\n",
        "- Pandas can consume data returned by **web APIs**, often in JSON format.\n",
        "- Steps typically involve:\n",
        "  - Sending a request via libraries like `requests` or `httpx`\n",
        "  - Parsing the JSON response\n",
        "  - Using `json_normalize` to flatten nested structures into a DataFrame\n",
        "\n",
        "üìà **Use cases**:\n",
        "- Real-time analytics (stock prices, weather data, social media feeds)\n",
        "- Combining multiple API responses into a single dataset\n",
        "- Automating periodic data ingestion for dashboards or reports\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- APIs are a common source of **dynamic, up-to-date data**.\n",
        "- Proper handling of **authentication**, **rate limits**, and **nested JSON** is crucial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d2ac0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
        "\n",
        "with urllib.request.urlopen(url) as response:\n",
        "    data = response.read()  # read the raw bytes\n",
        "    json_data = json.loads(data)  # parse JSON\n",
        "\n",
        "df_api = pd.DataFrame(json_data)\n",
        "\n",
        "# log\n",
        "df_api.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d3d9503",
      "metadata": {},
      "outputs": [],
      "source": [
        "# handle nested JSON with json_normalize\n",
        "url = \"https://jsonplaceholder.typicode.com/users\"\n",
        "\n",
        "with urllib.request.urlopen(url) as response:\n",
        "    data = response.read()  # read the raw bytes\n",
        "    json_data = json.loads(data)  # parse JSON\n",
        "\n",
        "df_users = pd.json_normalize(json_data, sep=\"_\")\n",
        "print(df_users.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "983acaeb",
      "metadata": {},
      "source": [
        "## <a id='toc2_7_'></a>[Performance and Best Practices](#toc0_)\n",
        "\n",
        "- When working with data I/O, choosing the right format and strategy can **significantly improve performance** and reduce memory usage.\n",
        "- Pandas provides tools and options to handle large datasets efficiently.\n",
        "\n",
        "üìù **Docs**:  \n",
        "- Memory usage and dtypes: [pandas.pydata.org/docs/user_guide/basics.html#dtypes](https://pandas.pydata.org/docs/user_guide/basics.html#dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d63547a",
      "metadata": {},
      "source": [
        "### <a id='toc2_7_1_'></a>[Memory Usage Considerations](#toc0_)\n",
        "\n",
        "- Large datasets can easily exceed available RAM.\n",
        "- Strategies to reduce memory footprint:\n",
        "  - Load only **needed columns** (`usecols`)\n",
        "  - Specify **dtypes** to avoid default `object` or `float64`\n",
        "  - Process data in **chunks**\n",
        "  - Drop unnecessary columns early\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- Prevents crashes when working with large files.\n",
        "- Reduces computation time for operations on large datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5801aed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load only needed columns\n",
        "df_small = pd.read_csv(CSV_URL, usecols=[\"Film\", \"Genre\"])\n",
        "\n",
        "# log\n",
        "df_small.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980552bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# explicitly set data types to save memory\n",
        "df_typed = pd.read_csv(\n",
        "    CSV_URL,\n",
        "    dtype={\n",
        "        \"Audience score %\": \"int8\",\n",
        "        \"Profitability\": \"float32\",\n",
        "        \"Rotten Tomatoes %\": \"int8\",\n",
        "        \"Year\": \"int16\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# log\n",
        "df_typed.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae2c700",
      "metadata": {},
      "outputs": [],
      "source": [
        "# process large file in chunks\n",
        "chunk_iter = pd.read_csv(CSV_URL, chunksize=10)\n",
        "\n",
        "total_rows = 0\n",
        "for i, chunk in enumerate(chunk_iter, start=1):\n",
        "    total_rows += len(chunk)\n",
        "    mem_kb = chunk.memory_usage(deep=True).sum() / 1024\n",
        "    print(f\"chunk {i}: {len(chunk):>2} rows, ~{mem_kb:.2f} KB in memory\")\n",
        "\n",
        "print(f\"total rows processed: {total_rows}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b51d7a6",
      "metadata": {},
      "source": [
        "### <a id='toc2_7_2_'></a>[Choosing the Right File Format](#toc0_)\n",
        "\n",
        "- File formats differ in **speed, size, and compatibility**:\n",
        "  - **CSV:** human-readable, compatible, slower for large datasets\n",
        "  - **Excel:** widely used in business, not optimized for big data\n",
        "  - **JSON:** flexible, good for APIs, can be nested\n",
        "  - **Parquet:** fast columnar storage, highly efficient for analytics\n",
        "  - **HDF5:** efficient for numerical data and hierarchical structures\n",
        "\n",
        "üìà **Recommendation**:\n",
        "- Use **Parquet or HDF5** for large analytical datasets.\n",
        "- Use **CSV or Excel** for small, human-readable, or shared files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612301e7",
      "metadata": {},
      "source": [
        "### <a id='toc2_7_3_'></a>[Compression and Storage Trade-offs](#toc0_)\n",
        "\n",
        "- Pandas supports **compression** for many formats (`gzip`, `bz2`, `zip`).\n",
        "- Benefits:\n",
        "  - Reduces **disk space**\n",
        "  - Can sometimes **speed up I/O** by reading/writing smaller files\n",
        "- Trade-offs:\n",
        "  - Some compression types are slower to read/write\n",
        "  - Not all formats support all compression methods\n",
        "\n",
        "‚ÄºÔ∏è **Why it matters**:\n",
        "- Choosing the right combination of format and compression can save storage and improve workflow efficiency, especially in production pipelines.\n"
      ]
    }
  ],
  "metadata": {
    "author_email": "AmirhosseinHeydari78@gmail.com",
    "author_github": "https://github.com/mr-pylin",
    "author_name": "Amirhossein Heydari",
    "kernelspec": {
      "display_name": "pandas-workshop (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "origin_repo": "https://github.com/mr-pylin/pandas-workshop"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
